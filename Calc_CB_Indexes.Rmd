---
title: "Calculating Demographic Indexes for the Casco Bay Watershed"
uthor: "Curtis C. Bohlen, Casco Bay Estuary Partnership"
date: "July 27, 2022"
output:
  pdf_document:
    toc: true
    toc_depth: 2
---

<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:100px;right:50px;" />

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center',
                      fig.width = 5, fig.height = 4,
                      collapse = TRUE, comment = "#>")
```

# Introduction
CBEP, like other National Estuary Programs will receive additional funding to
support our programs via the "Bipartisan Infrastructure Law" signed into law 
last December.

EPA has recently released guidance for applying for those funds.  A core 
component of the guidance is that overall, the NEP program should comply with 
the White House's "Justice 40" initiative, which requires that "at least 40% of 
the benefits and investments from BIL funding flow to disadvantaged 
communities."

EPA suggested that we use the National-scale 
[EJSCREEN tools](https://www.epa.gov/ejscreen) to help identify "disadvantaged
communities" in our region. The EPA guidance goes on to suggest we focus on 
five demographic indicators:

*  Percent low-income;  

*  Percent linguistically isolated; 

*  Percent less than high school education;  

*  Percent unemployed; and  

*  Low life expectancy.

This notebook builds on the work in "Calc_Indexes.pdf" to calculate data for the
Casco Bay Watershed Census Tracts, and calculates how Casco Bay Census tracts
compare at National, Statewide, and Regional scales.

# Load Libraries
```{r libraries}
library(tidyverse)
library(GGally)
library(readr)
```

# Set Graphics Theme
This sets `ggplot()`graphics for no background, no grid lines, etc. in a clean
format suitable for (some) publications.

```{r set_theme}
theme_set(theme_classic())
```

# Load Data
## Folder References
I use folder references to allow limited indirection, thus making code from 
GitHub repositories more likely to run "out of the box".  

```{r folder_refs}
data_folder <- "Original_Data"
dir.create(file.path(getwd(), 'figures'), showWarnings = FALSE)
```

I use the "Original_Data" folder to retain data in the form originally
downloaded.  That minimizes the chances of inadvertently modifying the source 
data. All data was accessed via EJScreen.  The 2021 EJSCREEN Data
was accessed on July 26, 2022, at https://gaftp.epa.gov/EJSCREEN/2021/.  I 
downloaded geodatabases, and open the geospatial data they contained in ArcGIS
and exported the tabular attribute data to CSV files.  That tabular CSV data is 
provided in the "Original Data" folder here.

The "figures" folder isolates "final" versions of any graphics I produce.  That
just makes it a bit easier to find  final products in what can sometimes be 
fairly large GitHub Repositories (although not here).

## Load Data
The Tabular data is quite extensive (over 165 MB), which poses some data access
challenges.  The raw CSV file contains 74001 records, and 166 columns.  Most,
but not all are numeric.  The Health data is slightly smaller in length, and has 
only a handful of relevant data columns, but it DOES include State ad County 
names, which are more convenient that the GEOID10 values to search.


```{r}
the_file <- "Casco_Tracts.txt"
the_path <- file.path(data_folder, the_file)

cb_data <- read_csv(the_path, n_max = 81,
                    col_types = c(paste(c(rep('-',2),  'c-',  rep('d',2),
                                          rep('-',7), 'd'), collapse = '')))

the_file <- "National_Draft_Indexes.csv"
the_data <- read_csv(the_file,
                     n_max = 74001,
                     col_types = c(rep(col_character(),3), 
                                   rep(col_double(), 23))) %>%
  mutate(PCA_Index_V1 = -PCA_Index_V1,
         PCA_Index_V2 = -PCA_Index_V2)
```

```{r}
cb_data <- cb_data %>%
  inner_join(the_data, by = 'GEOID10')
```

# Save Data
```{r}
write_csv(cb_data, "cb_tracts_indexes.csv")
```

# Calculate Thresholds
We have six different indexes, and we want threshold values for each at 
National, State, and Casco Bay Region levels.  It's convenient to automate
the calculations using a small function and the `map()` function.

```{r}
(nms <- names(cb_data)[25:30])
```
### Utility Function
This function calculates the 80th percentile (by default, anyway) of a named
data column from a data frame.  There is no error checking, so this is NOT
appropriate for programming with out more work.

```{r}
quantile_select_col <- function(.data, .col, .q = 0.8) {
  return(quantile(.data[,.col], .q, na.rm = TRUE))
}

the_data %>%
  quantile_select_col( "Index_1")
```
### Calculations
I calculate a named vector of threshold values at NAtional, State and Casco
Bay Regional levels.

```{r}
National <- map(nms, function(x) quantile_select_col(the_data, x))
National <- unlist(National) # Flatten List to numeric vector
names(National) <- nms       # Add Names
National
```

```{r}
Maine <- map(nms, 
             function(x) quantile_select_col(the_data[the_data$State == 'Maine',], x))
Maine <- unlist(Maine) # Flatten List to numeric vector
names(Maine) <- nms       # Add Names
Maine
```

```{r}
Region <- map(nms, function(x) quantile_select_col(cb_data, x))
Region <- unlist(Region) # Flatten List to numeric vector
names(Region) <- nms       # Add Names
Region
```

```{r}
thresholds <- bind_rows(National, Maine, Region, .id = 'Scale') %>%
  mutate(Scale = c('National', 'Maine', 'Region'))
```


## Calculate ThresholdExceedences
I use a similar functional programming approach for calculating whether specific
Casco Bay Census Tracts exceed each threshold.  Here I pass both a dataframe
and a names list or vector containing the thresholds. 

### Utility Function
```{r}
threshold_compare <- function(.data, .thresholds, .col) {
  return(.data[.col] > .thresholds[.col])
}
```

```{r}
National["Index_1"]
```
And we can now quickly demonstrate that no Casco Bay census Block exceeds that 
value.

```{r}
sum(threshold_compare(cb_data, National, "Index_1"), na.rm = TRUE)
```
And not =ne exceed the 80th percentile for Maine either.
```{r}
sum(threshold_compare(cb_data, Maine, "Index_1"), na.rm = TRUE)
```
### Calculations
```{r}
National_Exceeds <- map(as.list(nms), 
                        function(x) threshold_compare(cb_data, National, x))
National_Exceeds <- as.data.frame(National_Exceeds)
names(National_Exceeds) <- paste0('Ex_Nat_', nms)
unlist(map(National_Exceeds, sum, na.rm = TRUE))
```

```{r}
Maine_Exceeds <- map(as.list(nms), 
                     function(x) threshold_compare(cb_data, Maine, x))
Maine_Exceeds <- as.data.frame(Maine_Exceeds)
names(Maine_Exceeds) <- paste0('Ex_Maine_', nms)
unlist(map(Maine_Exceeds, sum, na.rm = TRUE))
```

test <- map(as.list(nms), function(x) threshold_compare(cb_data, Region, x))
names(test) <- nms
test[[1]][1:5]
test <- as.data.frame(test)
test

```{r}
CB_Exceeds <- map(as.list(nms), 
                  function(x) threshold_compare(cb_data, Region, x))
CB_Exceeds <- as.data.frame(CB_Exceeds)
names(CB_Exceeds) <- paste0('Ex_CB_', nms)
unlist(map(CB_Exceeds, sum, na.rm = TRUE))
```

# Combine Data
```{r}
cb_data <- cb_data %>%
  bind_cols(National_Exceeds, Maine_Exceeds, CB_Exceeds)
```

I don't save that data, because it is actually easier to manage map colors 
without the hard thresholds.
